{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkA5a+1IOHFHKeqz4cba22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterpanw/Machine_learning/blob/main/torch_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsN_Qcm5_Ra1"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n6Y64Wpn_0YT",
        "outputId": "96a35697-e081-4de9-a078-92593dd5dab7"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6lOenL_ACu3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8POFNNUAiwS"
      },
      "source": [
        "#对训练集进行数据增强 包括左右翻转，灰度随机变换，并变为张量\n",
        "#测试集就不需要了 只需转换为张量即可\n",
        "transform=transforms.Compose(\n",
        "    [\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomGrayscale(),\n",
        "     transforms.ToTensor()])\n",
        "transform1=transforms.Compose(\n",
        "    [\n",
        "     transforms.ToTensor()])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u3j7P4WBeJv"
      },
      "source": [
        "#下载数据\n",
        "trainset=torchvision.datasets.FashionMNIST(root='./data',train=True,\n",
        "                   download=True,transform=transform)\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=100,\n",
        "                   shuffle=True,num_workers=2)\n",
        "testset=torchvision.datasets.FashionMNIST(root='./data',train=False,\n",
        "                   download=True,transform=transform1)\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=50,\n",
        "                   shuffle=False,num_workers=2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA-bON4bDLlw"
      },
      "source": [
        "classes = ('T-shirt', 'Trouser', 'Pullover', 'Dress',\n",
        "           'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb82vsBhGSqz"
      },
      "source": [
        "#搭建网络\n",
        "class Net(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,64,1,padding=1)\n",
        "        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3,padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc5 = nn.Linear(128*8*8,512)\n",
        "        self.drop1 = nn.Dropout2d()\n",
        "        self.fc6 = nn.Linear(512,10)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        #print(\" x shape \",x.size())\n",
        "        x = x.view(-1,128*8*8)\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc6(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_sgd(self,device,epochs=100):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
        "\n",
        "        path = 'weights.tar'\n",
        "        initepoch = 0\n",
        "\n",
        "        if os.path.exists(path) is not True:\n",
        "            loss = nn.CrossEntropyLoss()\n",
        "            # optimizer = optim.SGD(self.parameters(),lr=0.01)\n",
        "\n",
        "        else:\n",
        "            checkpoint = torch.load(path)\n",
        "            self.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            initepoch = checkpoint['epoch']\n",
        "            loss = checkpoint['loss']\n",
        "\n",
        "\n",
        "        for epoch in range(initepoch,epochs):  # loop over the dataset multiple times\n",
        "            timestart = time.time()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device),labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = self(inputs)\n",
        "                l = loss(outputs, labels)\n",
        "                l.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += l.item()\n",
        "                # print(\"i \",i)\n",
        "                if i % 500 == 499:  # print every 500 mini-batches\n",
        "                    print('[%d, %5d] loss: %.4f' %\n",
        "                          (epoch, i, running_loss / 500))\n",
        "                    running_loss = 0.0\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                    print('Accuracy of the network on the %d tran images: %.3f %%' % (total,\n",
        "                            100.0 * correct / total))\n",
        "                    total = 0\n",
        "                    correct = 0\n",
        "                    torch.save({'epoch':epoch,\n",
        "                                'model_state_dict':net.state_dict(),\n",
        "                                'optimizer_state_dict':optimizer.state_dict(),\n",
        "                                'loss':loss\n",
        "                                },path)\n",
        "\n",
        "            print('epoch %d cost %3f sec' %(epoch,time.time()-timestart))\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "    def test(self,device):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                images, labels = data\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = self(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Accuracy of the network on the 10000 test images: %.3f %%' % (\n",
        "                100.0 * correct / total))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUNnR6cwHcix",
        "outputId": "a7285327-21ce-46e5-e19e-e83c0f4483fd"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net = Net()\n",
        "    net = net.to(device)\n",
        "    net.train_sgd(device,30)\n",
        "    net.test(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0,   499] loss: 0.2900\n",
            "Accuracy of the network on the 100 tran images: 91.000 %\n",
            "epoch 0 cost 24.886730 sec\n",
            "[1,   499] loss: 0.2434\n",
            "Accuracy of the network on the 100 tran images: 87.000 %\n",
            "epoch 1 cost 24.898417 sec\n",
            "[2,   499] loss: 0.2190\n",
            "Accuracy of the network on the 100 tran images: 88.000 %\n",
            "epoch 2 cost 25.131361 sec\n",
            "[3,   499] loss: 0.1971\n",
            "Accuracy of the network on the 100 tran images: 93.000 %\n",
            "epoch 3 cost 24.918609 sec\n",
            "[4,   499] loss: 0.1817\n",
            "Accuracy of the network on the 100 tran images: 91.000 %\n",
            "epoch 4 cost 24.974836 sec\n",
            "[5,   499] loss: 0.1676\n",
            "Accuracy of the network on the 100 tran images: 95.000 %\n",
            "epoch 5 cost 24.885029 sec\n",
            "[6,   499] loss: 0.1550\n",
            "Accuracy of the network on the 100 tran images: 94.000 %\n",
            "epoch 6 cost 24.899967 sec\n",
            "[7,   499] loss: 0.1413\n",
            "Accuracy of the network on the 100 tran images: 96.000 %\n",
            "epoch 7 cost 24.975025 sec\n",
            "[8,   499] loss: 0.1342\n",
            "Accuracy of the network on the 100 tran images: 93.000 %\n",
            "epoch 8 cost 24.911045 sec\n",
            "[9,   499] loss: 0.1214\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 9 cost 24.936462 sec\n",
            "[10,   499] loss: 0.1112\n",
            "Accuracy of the network on the 100 tran images: 95.000 %\n",
            "epoch 10 cost 24.917646 sec\n",
            "[11,   499] loss: 0.1020\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 11 cost 24.943627 sec\n",
            "[12,   499] loss: 0.0952\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 12 cost 24.916075 sec\n",
            "[13,   499] loss: 0.0894\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 13 cost 24.937753 sec\n",
            "[14,   499] loss: 0.0840\n",
            "Accuracy of the network on the 100 tran images: 94.000 %\n",
            "epoch 14 cost 25.037846 sec\n",
            "[15,   499] loss: 0.0751\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 15 cost 24.948253 sec\n",
            "[16,   499] loss: 0.0707\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 16 cost 24.929020 sec\n",
            "[17,   499] loss: 0.0638\n",
            "Accuracy of the network on the 100 tran images: 99.000 %\n",
            "epoch 17 cost 24.843587 sec\n",
            "[18,   499] loss: 0.0625\n",
            "Accuracy of the network on the 100 tran images: 95.000 %\n",
            "epoch 18 cost 24.912395 sec\n",
            "[19,   499] loss: 0.0557\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 19 cost 24.921447 sec\n",
            "[20,   499] loss: 0.0531\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 20 cost 24.929163 sec\n",
            "[21,   499] loss: 0.0477\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 21 cost 24.850031 sec\n",
            "[22,   499] loss: 0.0455\n",
            "Accuracy of the network on the 100 tran images: 97.000 %\n",
            "epoch 22 cost 24.829861 sec\n",
            "[23,   499] loss: 0.0446\n",
            "Accuracy of the network on the 100 tran images: 99.000 %\n",
            "epoch 23 cost 24.834280 sec\n",
            "[24,   499] loss: 0.0390\n",
            "Accuracy of the network on the 100 tran images: 99.000 %\n",
            "epoch 24 cost 24.824232 sec\n",
            "[25,   499] loss: 0.0375\n",
            "Accuracy of the network on the 100 tran images: 96.000 %\n",
            "epoch 25 cost 24.928258 sec\n",
            "[26,   499] loss: 0.0369\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 26 cost 24.897172 sec\n",
            "[27,   499] loss: 0.0338\n",
            "Accuracy of the network on the 100 tran images: 100.000 %\n",
            "epoch 27 cost 25.010691 sec\n",
            "[28,   499] loss: 0.0332\n",
            "Accuracy of the network on the 100 tran images: 100.000 %\n",
            "epoch 28 cost 24.954798 sec\n",
            "[29,   499] loss: 0.0301\n",
            "Accuracy of the network on the 100 tran images: 98.000 %\n",
            "epoch 29 cost 24.979814 sec\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 92.520 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxIA24hcHqd0"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}